{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pm4py\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#from mlxtend.preprocessing import TransactionEncoder\n",
    "#from mlxtend.frequent_patterns import apriori, association_rules\n",
    "from pm4py.objects.log.importer.xes import importer\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Reading And Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Event names where start and end times are the same:\n",
      "['emergency_patient', 'releasing', 'patient_referal', 'time_for_intake', 'patient_left_due_to_long_wait']\n"
     ]
    }
   ],
   "source": [
    "# Read the event log CSV file\n",
    "csv_file_path = 'event_log.csv'  # Replace with your CSV file path\n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Convert 'start_time' column to datetime if it's not already\n",
    "df['start_time'] = pd.to_datetime(df['start_time'])\n",
    "\n",
    "# Find events where start_time equals end_time\n",
    "events_same_start_end = df[df['start_time'] == df['completion_time']]\n",
    "\n",
    "# Get list of event names where start and end times are the same\n",
    "event_names_same_start_end = events_same_start_end['event_label'].unique().tolist()\n",
    "\n",
    "# Display or further process event names where start and end times are the same\n",
    "print(\"Event names where start and end times are the same:\")\n",
    "print(event_names_same_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the original CSV file\n",
    "df = pd.read_csv('event_log.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 90430 entries, 0 to 90429\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   case_id          90430 non-null  int64 \n",
      " 1   task_id          90430 non-null  int64 \n",
      " 2   event_label      90430 non-null  object\n",
      " 3   resource         21234 non-null  object\n",
      " 4   start_time       90430 non-null  object\n",
      " 5   completion_time  90430 non-null  object\n",
      " 6   diagnosis        12128 non-null  object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 4.8+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# Check for null cells in the entire DataFrame\n",
    "null_cells = df.isnull()\n",
    "print(True in null_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case_id</th>\n",
       "      <th>task_id</th>\n",
       "      <th>event_label</th>\n",
       "      <th>resource</th>\n",
       "      <th>start_time</th>\n",
       "      <th>completion_time</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>emergency_patient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 00:57:24.551134</td>\n",
       "      <td>2018-01-01 00:57:24.551134</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>ER_treatment</td>\n",
       "      <td>ER_PRACTITIONER3</td>\n",
       "      <td>2018-01-01 00:57:24.551134</td>\n",
       "      <td>2018-01-01 02:30:33.427464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>releasing</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 02:30:33.427464</td>\n",
       "      <td>2018-01-01 02:30:33.427464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>emergency_patient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 03:03:14.397047</td>\n",
       "      <td>2018-01-01 03:03:14.397047</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>ER_treatment</td>\n",
       "      <td>ER_PRACTITIONER3</td>\n",
       "      <td>2018-01-01 03:03:14.397047</td>\n",
       "      <td>2018-01-01 04:20:35.559032</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>emergency_patient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 06:16:04.316754</td>\n",
       "      <td>2018-01-01 06:16:04.316754</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>emergency_patient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 07:06:12.075649</td>\n",
       "      <td>2018-01-01 07:06:12.075649</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>emergency_patient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 07:55:07.334212</td>\n",
       "      <td>2018-01-01 07:55:07.334212</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>emergency_patient</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-01-01 08:16:09.204766</td>\n",
       "      <td>2018-01-01 08:16:09.204766</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>ER_treatment</td>\n",
       "      <td>ER_PRACTITIONER3</td>\n",
       "      <td>2018-01-01 06:16:04.316754</td>\n",
       "      <td>2018-01-01 08:38:28.243767</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   case_id  task_id        event_label          resource  \\\n",
       "0        0        0  emergency_patient               NaN   \n",
       "1        0        2       ER_treatment  ER_PRACTITIONER3   \n",
       "2        0        3          releasing               NaN   \n",
       "3        1        1  emergency_patient               NaN   \n",
       "4        1        5       ER_treatment  ER_PRACTITIONER3   \n",
       "5        2        4  emergency_patient               NaN   \n",
       "6        3        7  emergency_patient               NaN   \n",
       "7        4        9  emergency_patient               NaN   \n",
       "8        5       11  emergency_patient               NaN   \n",
       "9        2        8       ER_treatment  ER_PRACTITIONER3   \n",
       "\n",
       "                   start_time             completion_time diagnosis  \n",
       "0  2018-01-01 00:57:24.551134  2018-01-01 00:57:24.551134       NaN  \n",
       "1  2018-01-01 00:57:24.551134  2018-01-01 02:30:33.427464       NaN  \n",
       "2  2018-01-01 02:30:33.427464  2018-01-01 02:30:33.427464       NaN  \n",
       "3  2018-01-01 03:03:14.397047  2018-01-01 03:03:14.397047        B2  \n",
       "4  2018-01-01 03:03:14.397047  2018-01-01 04:20:35.559032       NaN  \n",
       "5  2018-01-01 06:16:04.316754  2018-01-01 06:16:04.316754        B1  \n",
       "6  2018-01-01 07:06:12.075649  2018-01-01 07:06:12.075649       NaN  \n",
       "7  2018-01-01 07:55:07.334212  2018-01-01 07:55:07.334212        B2  \n",
       "8  2018-01-01 08:16:09.204766  2018-01-01 08:16:09.204766        B1  \n",
       "9  2018-01-01 06:16:04.316754  2018-01-01 08:38:28.243767       NaN  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now see that we have a lot of unneccesary columns so we will remove them and we will organize the events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Apply the transformation\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_event_label_and_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Display the transformed dataframe\u001b[39;00m\n\u001b[0;32m     15\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m, in \u001b[0;36mtransform_event_label_and_resource\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_event_label_and_resource\u001b[39m(df):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m----> 4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresource\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNone\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;66;03m# Change the event_label to the resource value\u001b[39;00m\n\u001b[0;32m      6\u001b[0m             df\u001b[38;5;241m.\u001b[39mat[index, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_label\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresource\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m             \u001b[38;5;66;03m# Change the resource to 'None'\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1017\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[1;32m-> 1017\u001b[0m     \u001b[43mcheck_dict_or_set_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1018\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mEllipsis\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexing.py:2678\u001b[0m, in \u001b[0;36mcheck_dict_or_set_indexers\u001b[1;34m(key)\u001b[0m\n\u001b[0;32m   2666\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2667\u001b[0m \u001b[38;5;124;03m    Returns\u001b[39;00m\n\u001b[0;32m   2668\u001b[0m \u001b[38;5;124;03m    -------\u001b[39;00m\n\u001b[0;32m   2669\u001b[0m \u001b[38;5;124;03m    bool\u001b[39;00m\n\u001b[0;32m   2670\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m   2672\u001b[0m         obj\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2673\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2674\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m (obj\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mstep \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   2675\u001b[0m     )\n\u001b[1;32m-> 2678\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_dict_or_set_indexers\u001b[39m(key) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2679\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m \u001b[38;5;124;03m    Check if the indexer is or contains a dict or set, which is no longer allowed.\u001b[39;00m\n\u001b[0;32m   2681\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2683\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mset\u001b[39m)\n\u001b[0;32m   2684\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m)\n\u001b[0;32m   2685\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mset\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   2686\u001b[0m     ):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Define the function to modify the dataframe in place\n",
    "# def transform_event_label_and_resource(df):\n",
    "#     for index, row in df.iterrows():\n",
    "#         if row['resource'] != 'None':\n",
    "#             # Change the event_label to the resource value\n",
    "#             df.at[index, 'event_label'] = row['resource']\n",
    "#             # Change the resource to 'None'\n",
    "#             df.at[index, 'resource'] = 'None'\n",
    "#     return df\n",
    "\n",
    "# # Apply the transformation\n",
    "# df = transform_event_label_and_resource(df)\n",
    "\n",
    "# # Display the transformed dataframe\n",
    "# df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the resource column\n",
    "df = df.drop(columns=['resource'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the function to transform the dataframe\n",
    "# def split_start_completion(df):\n",
    "#     new_rows = []\n",
    "#     for index, row in df.iterrows():\n",
    "#         # Create the _start row\n",
    "#         if not row['event_label'] in event_names_same_start_end:\n",
    "#             start_row = row.copy()\n",
    "#             start_row['event_label'] = f\"{row['event_label']}\"\n",
    "#             new_rows.append(start_row)\n",
    "#         else:\n",
    "#             new_rows.append(row)\n",
    "    \n",
    "#     # Create the new dataframe without the completion_time column\n",
    "#     new_df = pd.DataFrame(new_rows).drop(columns=['completion_time'])\n",
    "    \n",
    "#     return new_df\n",
    "\n",
    "# # Apply the transformation\n",
    "# df = split_start_completion(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['task_id', 'diagnosis', 'completion_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = df.sort_values(['case_id', 'start_time'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the transformed dataframe to a new CSV file if needed\n",
    "sorted_df.to_csv('after_cleaning.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataframe for conversion to XES\n",
    "sorted_df.rename(columns={'case_id': 'case:concept:name', 'event_label': 'concept:name', 'start_time': 'time:timestamp'}, inplace=True)\n",
    "sorted_df['time:timestamp'] = pd.to_datetime(sorted_df['time:timestamp'])\n",
    "\n",
    "# Convert the dataframe to an XES event log\n",
    "event_log = pm4py.format_dataframe(sorted_df, case_id='case:concept:name', activity_key='concept:name', timestamp_key='time:timestamp')\n",
    "xes_event_log = pm4py.convert_to_event_log(event_log)\n",
    "\n",
    "# Save the XES event log to a file\n",
    "pm4py.write_xes(xes_event_log, 'event_log.xes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### we will split the data to train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the DataFrame into training and testing sets\n",
    "train, test = train_test_split(sorted_df, test_size=0.4, random_state=42)\n",
    "\n",
    "# Convert the dataframe to an XES event log\n",
    "event_log_train = pm4py.format_dataframe(train, case_id='case:concept:name', activity_key='concept:name', timestamp_key='time:timestamp')\n",
    "xes_event_log_train = pm4py.convert_to_event_log(event_log_train)\n",
    "\n",
    "# Save the XES event log to a file\n",
    "pm4py.write_xes(xes_event_log_train, 'event_log_train.xes')\n",
    "\n",
    "# Convert the dataframe to an XES event log\n",
    "event_log_test = pm4py.format_dataframe(test, case_id='case:concept:name', activity_key='concept:name', timestamp_key='time:timestamp')\n",
    "xes_event_log_test = pm4py.convert_to_event_log(event_log_train)\n",
    "\n",
    "# Save the XES event log to a file\n",
    "pm4py.write_xes(xes_event_log_train, 'event_log_test.xes')\n",
    "\n",
    "# Save the training and testing sets to separate CSV files\n",
    "train.to_csv('train_set.csv', index=False)\n",
    "test.to_csv('test_set.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "getting the Petri Net of the log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "applying the alpha algorithm on the xes file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "\n",
    "# Load the event log\n",
    "xes_event_log_train = pm4py.read_xes('event_log_train.xes')\n",
    "\n",
    "# Apply the Alpha Miner algorithm to discover a Petri net\n",
    "alpha_net, alpha_initial_marking, alpha_final_marking = alpha_miner.apply(xes_event_log_train)\n",
    "\n",
    "# Save the Petri net to a PNML file\n",
    "pm4py.write_pnml(alpha_net, alpha_initial_marking, alpha_final_marking, 'alpha_mined_petri_net.pnml')\n",
    "\n",
    "# Visualize the Petri net\n",
    "pm4py.view_petri_net(alpha_net, alpha_initial_marking, alpha_final_marking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inductive Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the event log\n",
    "xes_event_log_train = pm4py.read_xes('event_log_train.xes')\n",
    "\n",
    "# Apply the Inductive Miner algorithm to discover a Petri net\n",
    "net, initial_marking, final_marking = pm4py.discover_petri_net_inductive(xes_event_log_train)\n",
    "\n",
    "# Save the Petri net to a PNML file\n",
    "pm4py.write_pnml(net, initial_marking, final_marking, 'inductive_mined_petri_net.pnml')\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heuristic Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the event log\n",
    "xes_event_log_train = pm4py.read_xes('event_log_train.xes')\n",
    "\n",
    "# Apply the Heuristic Miner algorithm to discover a Petri net\n",
    "net, initial_marking, final_marking = pm4py.discover_petri_net_heuristics(xes_event_log_train)\n",
    "\n",
    "# Save the Petri net to a PNML file\n",
    "pm4py.write_pnml(net, initial_marking, final_marking, 'heuristic_mined_petri_net.pnml')\n",
    "pm4py.view_petri_net(net, initial_marking, final_marking)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rechability Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pm4py.algo.analysis.petri_net.reachability import algorithm as reachability_algorithm\n",
    "\n",
    "# Load the event log\n",
    "xes_event_log_train = pm4py.read_xes('event_log_train.xes')\n",
    "\n",
    "# Apply the Alpha Miner algorithm to discover a Petri net\n",
    "net, initial_marking, final_marking = pm4py.discover_petri_net_alpha(xes_event_log_train)\n",
    "\n",
    "# Get the reachability graph\n",
    "reachability_graph = reachability_algorithm.apply(net, initial_marking)\n",
    "\n",
    "# Visualize the reachability graph\n",
    "gviz = reachability_algorithm.view(reachability_graph)\n",
    "graphviz.Source(gviz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pm4py\n",
    "from pm4py.algo.evaluation.replay_fitness import algorithm as replay_fitness\n",
    "from pm4py.algo.evaluation.precision import algorithm as precision_evaluator\n",
    "from pm4py.algo.evaluation.generalization import algorithm as generalization_evaluator\n",
    "from pm4py.algo.evaluation.simplicity import algorithm as simplicity_evaluator\n",
    "\n",
    "# Load the training and testing event logs\n",
    "xes_event_log_train = pm4py.read_xes('event_log_train.xes')\n",
    "xes_event_log_test = pm4py.read_xes('event_log_test.xes')\n",
    "\n",
    "# Define a function to evaluate performance using the test log\n",
    "def evaluate_performance(log, net, initial_marking, final_marking):\n",
    "    # Fitness\n",
    "    fitness_value = replay_fitness.apply(log, net, initial_marking, final_marking)['averageFitness']\n",
    "    \n",
    "    # Precision\n",
    "    precision_value = precision_evaluator.apply(log, net, initial_marking, final_marking)\n",
    "    \n",
    "    # Generalization\n",
    "    generalization_value = generalization_evaluator.apply(log, net, initial_marking, final_marking)\n",
    "    \n",
    "    # Simplicity\n",
    "    simplicity_value = simplicity_evaluator.apply(net)\n",
    "    \n",
    "    return {\n",
    "        \"fitness\": fitness_value,\n",
    "        \"precision\": precision_value,\n",
    "        \"generalization\": generalization_value,\n",
    "        \"simplicity\": simplicity_value\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.inductive import algorithm as inductive_miner\n",
    "\n",
    "# Apply the Inductive Miner algorithm to discover a Petri net\n",
    "inductive_net, inductive_initial_marking, inductive_final_marking = pm4py.discover_petri_net_inductive(xes_event_log_train)\n",
    "\n",
    "# Save the Petri net to a PNML file\n",
    "pm4py.write_pnml(inductive_net, inductive_initial_marking, inductive_final_marking, 'inductive_mined_petri_net.pnml')\n",
    "inductive_performance = evaluate_performance(xes_event_log_test, inductive_net, inductive_initial_marking, inductive_final_marking)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"\\nInductive Miner Performance on Test Log:\")\n",
    "print(inductive_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.heuristics import algorithm as heuristics_miner\n",
    "\n",
    "# Heuristic Miner\n",
    "heuristic_net, heuristic_initial_marking, heuristic_final_marking = heuristics_miner.apply(xes_event_log_train)\n",
    "heuristic_performance = evaluate_performance(xes_event_log_test, heuristic_net, heuristic_initial_marking, heuristic_final_marking)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"\\nHeuristic Miner Performance on Test Log:\")\n",
    "print(heuristic_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bcc82d05b444f71901e01b16aa2360f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ca5ef65f538401d8180c8f151e2512c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "replaying log with TBR, completed traces ::   0%|          | 0/195 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Missing Tokens: 0\n",
      "Total Remaining Tokens: 0\n",
      "Total Produced Tokens: 13960\n",
      "Total Consumed Tokens: 13960\n",
      "Fitness: 1.0000\n"
     ]
    }
   ],
   "source": [
    "import pm4py\n",
    "from pm4py.algo.conformance.tokenreplay import algorithm as token_replay\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Read the event log\n",
    "log_2 = pm4py.read_xes('event_log_train.xes')\n",
    "\n",
    "# Step 2: Discover a Petri net using inductive miner\n",
    "net, im, fm = pm4py.discover_petri_net_inductive(log_2)\n",
    "\n",
    "# Step 3: Perform token-based replay conformance checking\n",
    "replay_result = token_replay.apply(log_2, net, im, fm)\n",
    "\n",
    "# Convert replay results to a DataFrame manually\n",
    "records = []\n",
    "for case in replay_result:\n",
    "    missing_tokens = case['missing_tokens']\n",
    "    remaining_tokens = case['remaining_tokens']\n",
    "    produced_tokens = case['produced_tokens']\n",
    "    consumed_tokens = case['consumed_tokens']\n",
    "    fit_traces = case['trace_is_fit']\n",
    "    \n",
    "    records.append({\n",
    "        'missing_tokens': missing_tokens,\n",
    "        'remaining_tokens': remaining_tokens,\n",
    "        'produced_tokens': produced_tokens,\n",
    "        'consumed_tokens': consumed_tokens,\n",
    "        'fit_traces': fit_traces\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Calculate the total missing, extra, produced, and consumed tokens\n",
    "total_missing_tokens = df['missing_tokens'].sum()\n",
    "total_extra_tokens = df['remaining_tokens'].sum()\n",
    "total_produced_tokens = df['produced_tokens'].sum()\n",
    "total_consumed_tokens = df['consumed_tokens'].sum()\n",
    "\n",
    "# Calculate the fitness using the specified formula\n",
    "\n",
    "\n",
    "# Print the summary\n",
    "print(f\"Total Missing Tokens: {total_missing_tokens}\")\n",
    "print(f\"Total Remaining Tokens: {total_extra_tokens}\")\n",
    "print(f\"Total Produced Tokens: {total_produced_tokens}\")\n",
    "print(f\"Total Consumed Tokens: {total_consumed_tokens}\")\n",
    "\n",
    "fitness = 0.5 * (1 - total_missing_tokens / total_produced_tokens) + 0.5 * (1 - total_extra_tokens / total_consumed_tokens)\n",
    "print(f\"Fitness: {fitness:.4f}\")\n",
    "\n",
    "# Export the conformance checking results to a CSV file\n",
    "csv_file_path = 'token_based_conformance_checking_results.csv'\n",
    "df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "# Save the summary results to a text file\n",
    "with open('conformance_summary.txt', 'w') as file:\n",
    "    file.write(f\"Total Missing Tokens: {total_missing_tokens}\\n\")\n",
    "    file.write(f\"Total Extra Tokens: {total_extra_tokens}\\n\")\n",
    "    file.write(f\"Total Produced Tokens: {total_produced_tokens}\\n\")\n",
    "    file.write(f\"Total Consumed Tokens: {total_consumed_tokens}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pm4py.algo.discovery.alpha import algorithm as alpha_miner\n",
    "\n",
    "# Apply the Alpha Miner algorithm to discover a Petri net\n",
    "alpha_net, alpha_initial_marking, alpha_final_marking = alpha_miner.apply(xes_event_log_train)\n",
    "\n",
    "# Evaluate performance on the test log\n",
    "alpha_performance = evaluate_performance(xes_event_log_test, alpha_net, alpha_initial_marking, alpha_final_marking)\n",
    "\n",
    "# Print the performance metrics\n",
    "print(\"\\nAlpha Miner Performance on Test Log:\")\n",
    "print(alpha_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discovery part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we get from here what are the activities we start from (referal, emergency_patients and the er_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = importer.apply('event_log.xes')\n",
    "\n",
    "log_start = pm4py.get_start_activities(log)\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "df = pd.DataFrame(list(log_start.items()), columns=['Activity', 'Count'])\n",
    "\n",
    "# Specify the file path for CSV\n",
    "csv_file_path = 'log_start_activities.csv'\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv(csv_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = importer.apply('event_log.xes')\n",
    "\n",
    "# Get end activities\n",
    "log_end = pm4py.get_end_activities(log)\n",
    "# Convert dictionary to DataFrame\n",
    "end_df = pd.DataFrame(list(log_end.items()), columns=['Activity', 'Count'])\n",
    "\n",
    "# Specify the file path for end activities CSV\n",
    "csv_file_path_end = 'log_end_activities.csv'\n",
    "# Export end activities to CSV\n",
    "end_df.to_csv(csv_file_path_end, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we will create traces of our activites "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping event labels to numbers\n",
    "event_label_to_number = {}\n",
    "current_number = 1  # Start with number 1\n",
    "\n",
    "for event_label in sorted_df['concept:name'].unique():\n",
    "    event_label_to_number[event_label] = current_number\n",
    "    current_number += 1\n",
    "\n",
    "# Create a new column 'number' based on event_label\n",
    "sorted_df['number'] = sorted_df['concept:name'].map(event_label_to_number)\n",
    "\n",
    "# Group by case_id and concatenate numbers into traces with spaces\n",
    "traces = sorted_df.groupby('case:concept:name')['number'].apply(lambda x: ' '.join(map(str, x))).reset_index()\n",
    "\n",
    "# Count occurrences of each trace\n",
    "trace_counts = traces['number'].value_counts().reset_index()\n",
    "trace_counts.columns = ['trace', 'count']\n",
    "\n",
    "# Specify the file path for CSV\n",
    "csv_file_path_trace_counts = 'trace_counts.csv'\n",
    "csv_file_path_case_traces = 'case_traces.csv'\n",
    "\n",
    "# Export trace counts to CSV\n",
    "trace_counts.to_csv(csv_file_path_trace_counts, index=False)\n",
    "\n",
    "# Export case_id and trace to CSV\n",
    "traces.columns = ['case_id', 'trace']\n",
    "traces.to_csv(csv_file_path_case_traces, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the trace counts from CSV\n",
    "trace_counts = pd.read_csv('trace_counts.csv')\n",
    "\n",
    "def remove_consecutive_duplicates(trace):\n",
    "    numbers = trace.split()\n",
    "    result = [numbers[0]]\n",
    "    for num in numbers[1:]:\n",
    "        if num != result[-1]:\n",
    "            result.append(num)\n",
    "    return ' '.join(result)\n",
    "\n",
    "# Apply the function to remove consecutive duplicates\n",
    "trace_counts['trace'] = trace_counts['trace'].apply(remove_consecutive_duplicates)\n",
    "\n",
    "# Group by the trace and sum the counts\n",
    "deduplicated_trace_counts = trace_counts.groupby('trace')['count'].sum().reset_index()\n",
    "\n",
    "# Sort the DataFrame by count in descending order\n",
    "deduplicated_trace_counts = deduplicated_trace_counts.sort_values(by='count', ascending=False)\n",
    "\n",
    "# Export the deduplicated and sorted trace counts to CSV\n",
    "csv_file_path_deduplicated = 'trace_counts.csv'\n",
    "deduplicated_trace_counts.to_csv(csv_file_path_deduplicated, index=False)\n",
    "\n",
    "# Select the top 7 most frequent traces\n",
    "top_traces = deduplicated_trace_counts.head(10)\n",
    "\n",
    "# Create a bar chart of the top 7 trace counts\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(top_traces['trace'], top_traces['count'], color='skyblue')\n",
    "plt.xlabel('Trace')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Most Frequent Trace Counts')\n",
    "plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to make room for rotated x-axis labels\n",
    "\n",
    "# Save the bar chart to a file\n",
    "bar_chart_path = 'top_trace_counts_bar_chart.png'\n",
    "plt.savefig(bar_chart_path)\n",
    "\n",
    "# Display the bar chart\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(event_label_to_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Your dictionary\n",
    "data = {\n",
    "    'emergency_patient': 1, 'ER_PRACTITIONER3': 2, 'releasing': 3, 'B_BED3': 4, 'B_BED25': 5, 'ER_PRACTITIONER4': 6,\n",
    "    'ER_PRACTITIONER5': 7, 'B_BED26': 8, 'ER_PRACTITIONER6': 9, 'B_BED4': 10, 'patient_referal': 11, 'time_for_intake': 12,\n",
    "    'INTAKE3': 13, 'OR1': 14, 'A_BED20': 15, 'INTAKE2': 16, 'OR4': 17, 'B_BED11': 18, 'INTAKE1': 19, 'OR2': 20, 'A_BED1': 21,\n",
    "    'INTAKE4': 22, 'OR3': 23, 'A_BED15': 24, 'B_BED17': 25, 'B_BED5': 26, 'OR5': 27, 'A_BED16': 28, 'B_BED31': 29, 'A_BED14': 30,\n",
    "    'B_BED18': 31, 'A_BED8': 32, 'B_BED35': 33, 'A_BED18': 34, 'A_BED17': 35, 'B_BED32': 36, 'B_BED27': 37, 'B_BED12': 38,\n",
    "    'A_BED19': 39, 'A_BED21': 40, 'B_BED8': 41, 'A_BED22': 42, 'A_BED2': 43, 'A_BED30': 44, 'A_BED3': 45, 'A_BED28': 46,\n",
    "    'B_BED33': 47, 'A_BED23': 48, 'B_BED37': 49, 'A_BED4': 50, 'A_BED9': 51, 'B_BED19': 52, 'B_BED6': 53, 'B_BED30': 54,\n",
    "    'B_BED28': 55, 'B_BED9': 56, 'B_BED7': 57, 'B_BED29': 58, 'B_BED21': 59, 'B_BED38': 60, 'A_BED29': 61, 'A_BED24': 62,\n",
    "    'B_BED34': 63, 'B_BED13': 64, 'B_BED40': 65, 'B_BED24': 66, 'A_BED5': 67, 'B_BED14': 68, 'B_BED10': 69, 'A_BED10': 70,\n",
    "    'A_BED25': 71, 'patient_left_due_to_long_wait': 72, 'B_BED39': 73, 'B_BED23': 74, 'ER_PRACTITIONER7': 75, 'ER_PRACTITIONER1': 76,\n",
    "    'B_BED22': 77, 'B_BED36': 78, 'B_BED20': 79, 'B_BED1': 80, 'B_BED2': 81, 'B_BED15': 82, 'B_BED16': 83, 'ER_PRACTITIONER8': 84,\n",
    "    'ER_PRACTITIONER9': 85, 'A_BED11': 86, 'A_BED12': 87, 'A_BED26': 88, 'A_BED6': 89, 'A_BED13': 90, 'A_BED7': 91, 'A_BED27': 92,\n",
    "    'ER_PRACTITIONER2': 93\n",
    "}\n",
    "\n",
    "# Specify the file path for CSV\n",
    "csv_file_path = 'meaning.csv'\n",
    "\n",
    "# Writing the dictionary to a CSV file\n",
    "with open(csv_file_path, 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(['Key', 'Value'])  # Writing header\n",
    "    for key, value in data.items():\n",
    "        writer.writerow([key, value])\n",
    "\n",
    "print(f\"Data has been written to {csv_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of events per case\n",
    "case_event_counts = sorted_df['case:concept:name'].value_counts()\n",
    "\n",
    "# Plot the distribution of case lengths\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(case_event_counts, kde=True, color='purple')\n",
    "plt.title('Distribution of Events per Case')\n",
    "plt.xlabel('Number of Events per Case')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n",
    "# Display the case event counts\n",
    "print(case_event_counts.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a few cases to visualize\n",
    "sample_cases = sorted_df[sorted_df['case:concept:name'].isin(sorted_df['case:concept:name'].unique()[:5])]\n",
    "\n",
    "# Plot the event sequences for the selected cases\n",
    "plt.figure(figsize=(15, 8))\n",
    "sns.lineplot(x='time:timestamp', y='concept:name', hue='case:concept:name', data=sample_cases, marker='o')\n",
    "plt.title('Event Sequences for Sample Cases')\n",
    "plt.xlabel('Timestamp')\n",
    "plt.ylabel('Event Name')\n",
    "plt.legend(title='Case ID')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for sequence mining\n",
    "cases = sorted_df.groupby('case:concept:name')['concept:name'].apply(list).values\n",
    "\n",
    "# Encode the data\n",
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(cases).transform(cases)\n",
    "df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# Apply the apriori algorithm to find frequent itemsets\n",
    "frequent_itemsets = apriori(df, min_support=0.3, use_colnames=True)\n",
    "\n",
    "# Generate association rules\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "# Display the results\n",
    "print(frequent_itemsets)\n",
    "print(rules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Conformance Checking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reply Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aliganments Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
